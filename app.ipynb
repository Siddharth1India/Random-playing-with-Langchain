{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "LangChain is a framework for developing applications powered by language models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema\n",
    "Most interfaces in LangChain are text based because most language models support only \"text in, text out\" (if you are interested, we can have an hour long discussion where I can explain you everything about transformers architecture)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How LLMs work (practical approach, not theory)\n",
    "LLMs have 3 parts in prompt usually.\n",
    "1. System Chain Message\n",
    "    - A chat message representing information that should be instructions to the AI system.\n",
    "2. Human Chat Message\n",
    "    - A chat message representing information coming from a human interacting with the AI system.\n",
    "3. AI Chat Message\n",
    "    - A chat message representing information coming from the AI system.\n",
    "\n",
    "**__PRO_TIP__**: While making product, try to keep Human chat as low as possible because normal users are very bad at writing prompt and model give pretty bad results with bad prompts which will results into you replying to useless complaint emails.\n",
    "\n",
    "Other than that, one important thing I need to explain.\n",
    ">    - Our daily usage is when we use model with zero-shot learning but model give outstanding result when it is fine-tuned with some examples, usually known as few-shot learning. We use method of `Examples` for this where we provide model with input/output pairs so model can better understand what we are expecting. \n",
    "\n",
    "While talking about terminology, we use word `Document` for unstructured data. Documents have compeletely random formats so normal programming approach cannot get desired data from doc. That is why we use LLMs because they can really understand data and give response accordingly. Document has two parts, `page_content` and `metadata`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "LangChain deals with 3 types of models.\n",
    "1. Language Models\n",
    "2. Chat Models\n",
    "3. Text Embedding Models\n",
    "\n",
    "LLMs: <br>\n",
    "These models take a text string as input, and return a text string as output.<br><br>\n",
    "Chat Models:<br>\n",
    "These models are usually backed by a language model, but their APIs are more structured. Specifically, these models take a list of Chat Messages as input, and return a Chat Message.<br><br>\n",
    "Text Embedding Models<br>\n",
    "These models take text as input and return a list of floats.<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMs\n",
    "Large Language Models (LLMs) are a core component of LangChain. LangChain is not a provider of LLMs, but rather provides a standard interface through which you can interact with a variety of LLMs. The LLM class is a class designed for interfacing with LLMs. There are lots of LLM providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.\n",
    "<br>\n",
    "Here is code example:\n",
    "\n",
    "```\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-ada-001\", n=2, best_of=2)\n",
    "llm(\"Tell me a joke\")\n",
    "\n",
    "Output:\n",
    ">>> '\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'\n",
    "```\n",
    "Let's see how to pass multiple inputs:\n",
    "\n",
    "```\n",
    "llm_result = llm.generate([\"Tell me a joke\", \"Tell me a poem\"]*15)\n",
    "len(llm_result.generations)\n",
    "\n",
    "Output:\n",
    ">>> 30\n",
    "```\n",
    "As llm_result.generations return list, it can be accessed like standard list.\n",
    "\n",
    "If LLM provider provide usage data, LangChain will return that too but it will not work for all LLMs so consider it not standardized. As our usage is focused on OpenAI, it will work.\n",
    "\n",
    "```\n",
    "llm_result.llm_output\n",
    "```\n",
    "Results will look like:\n",
    "```\n",
    "{'token_usage': {'completion_tokens': 3903,\n",
    "  'total_tokens': 4023,\n",
    "  'prompt_tokens': 120}}\n",
    "```\n",
    "\n",
    "Further, you can also count number of tokens in query:\n",
    "\n",
    "```\n",
    "llm.get_num_tokens(\"what a joke\")\n",
    ">>> 3\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How (and why) to use the fake LLM\n",
    "We expose a fake LLM class that can be used for testing. This allows you to mock out calls to the LLM and simulate what would happen if the LLM responded in a certain way.\n",
    "\n",
    "Here is code example:\n",
    "\n",
    "```\n",
    "from langchain.llms.fake import FakeListLLM\n",
    "\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "tools = load_tools([\"python_repl\"])\n",
    "responses=[\n",
    "    \"Action: Python REPL\\nAction Input: print(2 + 2)\",\n",
    "    \"Final Answer: 4\"\n",
    "]\n",
    "llm = FakeListLLM(responses=responses)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "agent.run(\"whats 2 + 2\")\n",
    "\n",
    "Output:\n",
    "> Entering new AgentExecutor chain...\n",
    "Action: Python REPL\n",
    "Action Input: print(2 + 2)\n",
    "Observation: 4\n",
    "\n",
    "Thought:Final Answer: 4\n",
    "\n",
    "> Finished chain.\n",
    "\n",
    "'4'\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not writing about `LLM Wrappers`, `Cache LLM calls`, `Stream LLM and chat response` as my team does not require that ATM, will update if required in future. <br>\n",
    "Let's talk about token usage.\n",
    "\n",
    "```\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-002\", n=2, best_of=2)\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm(\"Tell me a joke\")\n",
    "    print(cb)\n",
    "```\n",
    "Output will be:\n",
    "\n",
    "```\n",
    ">>> Tokens Used: 42\n",
    "...  \tPrompt Tokens: 4\n",
    "... \tCompletion Tokens: 38\n",
    "... Successful Requests: 1\n",
    "... Total Cost (USD): $0.00084\n",
    "```\n",
    "\n",
    "Anything inside the context manager will get tracked. Here’s an example of using it to track multiple calls in sequence.\n",
    "\n",
    "```\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm(\"Tell me a joke\")\n",
    "    result2 = llm(\"Tell me a joke\")\n",
    "    print(cb.total_tokens)\n",
    "```\n",
    "Results:\n",
    "\n",
    "```\n",
    ">>> 91\n",
    "```\n",
    "\n",
    "Sometimes, our code will be using multiple steps, tracking is simple for that too.\n",
    "Here is code:\n",
    "\n",
    "```\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    response = agent.run(\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\")\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost (USD): ${cb.total_cost}\")\n",
    "```\n",
    "\n",
    "Results will go something like below:\n",
    "\n",
    "```\n",
    "> Entering new AgentExecutor chain...\n",
    " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\n",
    "Action: Search\n",
    "Action Input: \"Olivia Wilde boyfriend\"\n",
    "Observation: Sudeikis and Wilde's relationship ended in November 2020. Wilde was publicly served with court documents regarding child custody while she was presenting Don't Worry Darling at CinemaCon 2022. In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling.\n",
    "Thought: I need to find out Harry Styles' age.\n",
    "Action: Search\n",
    "Action Input: \"Harry Styles age\"\n",
    "Observation: 29 years\n",
    "Thought: I need to calculate 29 raised to the 0.23 power.\n",
    "Action: Calculator\n",
    "Action Input: 29^0.23\n",
    "Observation: Answer: 2.169459462491557\n",
    "\n",
    "Thought: I now know the final answer.\n",
    "Final Answer: Harry Styles, Olivia Wilde's boyfriend, is 29 years old and his age raised to the 0.23 power is 2.169459462491557.\n",
    "\n",
    "> Finished chain.\n",
    "Total Tokens: 1506\n",
    "Prompt Tokens: 1350\n",
    "Completion Tokens: 156\n",
    "Total Cost (USD): $0.03012\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integration guide\n",
    "Although entire guideline is for number of models, I am writing for the models my team will use, ie. OpenAI, Cohere, Hugging face Hub and Hugging face local pipelines\n",
    "\n",
    "##### Cohere\n",
    "\n",
    "```\n",
    ">>> pip install cohere\n",
    "```\n",
    "After installation, it is standard procedure.\n",
    "```\n",
    "# get a new token: https://dashboard.cohere.ai/\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "COHERE_API_KEY = getpass()\n",
    "\n",
    "from langchain.llms import Cohere\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm = Cohere(cohere_api_key=COHERE_API_KEY)\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "\n",
    "llm_chain.run(question)\n",
    "```\n",
    "\n",
    "Results will go like:\n",
    "\n",
    "```\n",
    "\" Let's start with the year that Justin Beiber was born. You know that he was born in 1994. We have to go back one year. 1993.\\n\\n1993 was the year that the Dallas Cowboys won the Super Bowl. They won over the Buffalo Bills in Super Bowl 26.\\n\\nNow, let's do it backwards. According to our information, the Green Bay Packers last won the Super Bowl in the 2010-2011 season. Now, we can't go back in time, so let's go from 2011 when the Packers won the Super Bowl, back to 1984. That is the year that the Packers won the Super Bowl over the Raiders.\\n\\nSo, we have the year that Justin Beiber was born, 1994, and the year that the Packers last won the Super Bowl, 2011, and now we have to go in the middle, 1986. That is the year that the New York Giants won the Super Bowl over the Denver Broncos. The Giants won Super Bowl 21.\\n\\nThe New York Giants won the Super Bowl in 1986. This means that the Green Bay Packers won the Super Bowl in 2011.\\n\\nDid you get it right? If you are still a bit confused, just try to go back to the question again and review the answer\"\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hugging Face Hub\n",
    "\n",
    "```\n",
    ">>> pip install huggingface_hub \n",
    "```\n",
    "After installation is done, following code will be useful.\n",
    "\n",
    "```\n",
    "# get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-token\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = getpass()\n",
    "\n",
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN\n",
    "\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "repo_id = \"google/flan-t5-xl\" # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=repo_id, model_kwargs={\"temperature\":0, \"max_length\":64})\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"Who won the FIFA World Cup in the year 1994? \"\n",
    "\n",
    "print(llm_chain.run(question))\n",
    "```\n",
    "\n",
    "This is how it will work, now let's see for specific model from Hub.\n",
    "\n",
    "```\n",
    "repo_id = \"stabilityai/stablelm-tuned-alpha-3b\"\n",
    "# Others include stabilityai/stablelm-base-alpha-3b\n",
    "# as well as 7B parameter versions\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=repo_id, model_kwargs={\"temperature\":0, \"max_length\":64})\n",
    "\n",
    "# Reuse the prompt and question from above.\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "print(llm_chain.run(question))\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hugging Face Local Pipelines\n",
    "Let's try for transformers model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ">>> pip install transformers\n",
    "```\n",
    "After installation is done, run following code:\n",
    "\n",
    "```\n",
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(model_id=\"bigscience/bloom-1b7\", task=\"text-generation\", model_kwargs={\"temperature\":0, \"max_length\":64})\n",
    "\n",
    "from langchain import PromptTemplate,  LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "print(llm_chain.run(question))\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    " First, we need to understand what is an electroencephalogram. An electroencephalogram is a recording of brain activity. It is a recording of brain activity that is made by placing electrodes on the scalp. The electrodes are placed\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OpenAI\n",
    "Let's talk about model which will be most used by us.\n",
    "\n",
    "```\n",
    "# get a token: https://platform.openai.com/account/api-keys\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "OPENAI_API_KEY = getpass()\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "\n",
    "llm_chain.run(question)\n",
    "\n",
    "```\n",
    "\n",
    "Output will be something like:\n",
    "\n",
    "```\n",
    "' Justin Bieber was born in 1994, so we are looking for the Super Bowl winner from that year. The Super Bowl in 1994 was Super Bowl XXVIII, and the winner was the Dallas Cowboys.'\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Model\n",
    "Chat models are a variation on language models. While chat models use language models under the hood, the interface they expose is a bit different. Rather than expose a “text in, text out” API, they expose an interface where “chat messages” are the inputs and outputs.\n",
    "\n",
    "**__NOTE__**: Chat interface is new and many changes are being done by LangChain at experiment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see example:\n",
    "\n",
    "```\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "```\n",
    "\n",
    "You can get chat completions by passing one or more messages to the chat model. The response will be a message. The types of messages currently supported in LangChain are `AIMessage`, `HumanMessage`, `SystemMessage`, and `ChatMessage` – ChatMessage takes in an arbitrary role parameter. Most of the time, you’ll just be dealing with HumanMessage, AIMessage, and SystemMessage\n",
    "\n",
    "```\n",
    "chat([HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")])\n",
    "\n",
    "```\n",
    "Output:\n",
    "\n",
    "```\n",
    "AIMessage(content=\"J'aime programmer.\", additional_kwargs={})\n",
    "```\n",
    "OpenAI’s chat model supports multiple messages as input.\n",
    "\n",
    "```\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "    HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")\n",
    "]\n",
    "chat(messages)\n",
    "```\n",
    "Output:\n",
    "\n",
    "```\n",
    "AIMessage(content=\"J'aime programmer.\", additional_kwargs={})\n",
    "\n",
    "```\n",
    "\n",
    "You can go one step further and generate completions for multiple sets of messages using `generate`. This returns an `LLMResult` with an additional `message` parameter.\n",
    "\n",
    "Here is code example:\n",
    "\n",
    "```\n",
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "        HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "        HumanMessage(content=\"Translate this sentence from English to French. I love artificial intelligence.\")\n",
    "    ],\n",
    "]\n",
    "result = chat.generate(batch_messages)\n",
    "result\n",
    "```\n",
    "\n",
    "Output will be like:\n",
    "\n",
    "```\n",
    "LLMResult(generations=[[ChatGeneration(text=\"J'aime programmer.\", generation_info=None, message=AIMessage(content=\"J'aime programmer.\", additional_kwargs={}))], [ChatGeneration(text=\"J'aime l'intelligence artificielle.\", generation_info=None, message=AIMessage(content=\"J'aime l'intelligence artificielle.\", additional_kwargs={}))]], llm_output={'token_usage': {'prompt_tokens': 71, 'completion_tokens': 18, 'total_tokens': 89}})\n",
    "```\n",
    "\n",
    "You can recover things like token usage from this LLMResult\n",
    "\n",
    "```\n",
    "result.llm_output\n",
    "\n",
    "```\n",
    "Output:\n",
    "\n",
    "```\n",
    "{'token_usage': {'prompt_tokens': 71,\n",
    "  'completion_tokens': 18,\n",
    "  'total_tokens': 89}}\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to use few shot examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I talked earlier, few-shot learning is very important and hence should be studied in detail to get models perform in better way. Models fine-tuned with few-shot learning will perform far betterc than zero-shot models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Alternating Human/AI messages\n",
    "The first way of doing few shot prompting relies on using alternating human/ai messages. See an example of this below.\n",
    "\n",
    "```\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "template=\"You are a helpful assistant that translates english to pirate.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "example_human = HumanMessagePromptTemplate.from_template(\"Hi\")\n",
    "example_ai = AIMessagePromptTemplate.from_template(\"Argh me mateys\")\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, example_human, example_ai, human_message_prompt])\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "# get a chat completion from the formatted messages\n",
    "chain.run(\"I love programming.\")\n",
    "```\n",
    "Output will be of form:\n",
    "\n",
    "```\n",
    "\"I be lovin' programmin', me hearty!\"\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### System Messages\n",
    "\n",
    "OpenAI provides an optional name parameter that they also recommend using in conjunction with system messages to do few shot prompting. Here is an example of how to do that below.\n",
    "\n",
    "```\n",
    "template=\"You are a helpful assistant that translates english to pirate.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "example_human = SystemMessagePromptTemplate.from_template(\"Hi\", additional_kwargs={\"name\": \"example_user\"})\n",
    "example_ai = SystemMessagePromptTemplate.from_template(\"Argh me mateys\", additional_kwargs={\"name\": \"example_assistant\"})\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, example_human, example_ai, human_message_prompt])\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "# get a chat completion from the formatted messages\n",
    "chain.run(\"I love programming.\")\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "\"I be lovin' programmin', me hearty.\"\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI integration with Chat Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "    HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")\n",
    "]\n",
    "chat(messages)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "AIMessage(content=\"J'aime programmer.\", additional_kwargs={})\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For current projects, we will not be using Text Embedding Models so not covering here."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

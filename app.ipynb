{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "LangChain is a framework for developing applications powered by language models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema\n",
    "Most interfaces in LangChain are text based because most language models support only \"text in, text out\" (if you are interested, we can have an hour long discussion where I can explain you everything about transformers architecture)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How LLMs work (practical approach, not theory)\n",
    "LLMs have 3 parts in prompt usually.\n",
    "1. System Chain Message\n",
    "    - A chat message representing information that should be instructions to the AI system.\n",
    "2. Human Chat Message\n",
    "    - A chat message representing information coming from a human interacting with the AI system.\n",
    "3. AI Chat Message\n",
    "    - A chat message representing information coming from the AI system.\n",
    "\n",
    "**__PRO_TIP__**: While making product, try to keep Human chat as low as possible because normal users are very bad at writing prompt and model give pretty bad results with bad prompts which will results into you replying to useless complaint emails.\n",
    "\n",
    "Other than that, one important thing I need to explain.\n",
    ">    - Our daily usage is when we use model with zero-shot learning but model give outstanding result when it is fine-tuned with some examples, usually known as few-shot learning. We use method of `Examples` for this where we provide model with input/output pairs so model can better understand what we are expecting. \n",
    "\n",
    "While talking about terminology, we use word `Document` for unstructured data. Documents have compeletely random formats so normal programming approach cannot get desired data from doc. That is why we use LLMs because they can really understand data and give response accordingly. Document has two parts, `page_content` and `metadata`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "LangChain deals with 3 types of models.\n",
    "1. Language Models\n",
    "2. Chat Models\n",
    "3. Text Embedding Models\n",
    "\n",
    "LLMs: <br>\n",
    "These models take a text string as input, and return a text string as output.<br><br>\n",
    "Chat Models:<br>\n",
    "These models are usually backed by a language model, but their APIs are more structured. Specifically, these models take a list of Chat Messages as input, and return a Chat Message.<br><br>\n",
    "Text Embedding Models<br>\n",
    "These models take text as input and return a list of floats.<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMs\n",
    "Large Language Models (LLMs) are a core component of LangChain. LangChain is not a provider of LLMs, but rather provides a standard interface through which you can interact with a variety of LLMs. The LLM class is a class designed for interfacing with LLMs. There are lots of LLM providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.\n",
    "<br>\n",
    "Here is code example:\n",
    "\n",
    "```\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-ada-001\", n=2, best_of=2)\n",
    "llm(\"Tell me a joke\")\n",
    "\n",
    "Output:\n",
    ">>> '\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'\n",
    "```\n",
    "Let's see how to pass multiple inputs:\n",
    "\n",
    "```\n",
    "llm_result = llm.generate([\"Tell me a joke\", \"Tell me a poem\"]*15)\n",
    "len(llm_result.generations)\n",
    "\n",
    "Output:\n",
    ">>> 30\n",
    "```\n",
    "As llm_result.generations return list, it can be accessed like standard list.\n",
    "\n",
    "If LLM provider provide usage data, LangChain will return that too but it will not work for all LLMs so consider it not standardized. As our usage is focused on OpenAI, it will work.\n",
    "\n",
    "```\n",
    "llm_result.llm_output\n",
    "```\n",
    "Results will look like:\n",
    "```\n",
    "{'token_usage': {'completion_tokens': 3903,\n",
    "  'total_tokens': 4023,\n",
    "  'prompt_tokens': 120}}\n",
    "```\n",
    "\n",
    "Further, you can also count number of tokens in query:\n",
    "\n",
    "```\n",
    "llm.get_num_tokens(\"what a joke\")\n",
    ">>> 3\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

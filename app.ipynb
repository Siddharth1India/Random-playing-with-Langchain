{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "LangChain is a framework for developing applications powered by language models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema\n",
    "Most interfaces in LangChain are text based because most language models support only \"text in, text out\" (if you are interested, we can have an hour long discussion where I can explain you everything about transformers architecture)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How LLMs work (practical approach, not theory)\n",
    "LLMs have 3 parts in prompt usually.\n",
    "1. System Chain Message\n",
    "    - A chat message representing information that should be instructions to the AI system.\n",
    "2. Human Chat Message\n",
    "    - A chat message representing information coming from a human interacting with the AI system.\n",
    "3. AI Chat Message\n",
    "    - A chat message representing information coming from the AI system.\n",
    "\n",
    "**__PRO_TIP__**: While making product, try to keep Human chat as low as possible because normal users are very bad at writing prompt and model give pretty bad results with bad prompts which will results into you replying to useless complaint emails.\n",
    "\n",
    "Other than that, one important thing I need to explain.\n",
    ">    - Our daily usage is when we use model with zero-shot learning but model give outstanding result when it is fine-tuned with some examples, usually known as few-shot learning. We use method of `Examples` for this where we provide model with input/output pairs so model can better understand what we are expecting. \n",
    "\n",
    "While talking about terminology, we use word `Document` for unstructured data. Documents have compeletely random formats so normal programming approach cannot get desired data from doc. That is why we use LLMs because they can really understand data and give response accordingly. Document has two parts, `page_content` and `metadata`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "LangChain deals with 3 types of models.\n",
    "1. Language Models\n",
    "2. Chat Models\n",
    "3. Text Embedding Models\n",
    "\n",
    "LLMs: <br>\n",
    "These models take a text string as input, and return a text string as output.<br><br>\n",
    "Chat Models:<br>\n",
    "These models are usually backed by a language model, but their APIs are more structured. Specifically, these models take a list of Chat Messages as input, and return a Chat Message.<br><br>\n",
    "Text Embedding Models<br>\n",
    "These models take text as input and return a list of floats.<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMs\n",
    "Large Language Models (LLMs) are a core component of LangChain. LangChain is not a provider of LLMs, but rather provides a standard interface through which you can interact with a variety of LLMs. The LLM class is a class designed for interfacing with LLMs. There are lots of LLM providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.\n",
    "<br>\n",
    "Here is code example:\n",
    "\n",
    "```\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-ada-001\", n=2, best_of=2)\n",
    "llm(\"Tell me a joke\")\n",
    "\n",
    "Output:\n",
    ">>> '\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'\n",
    "```\n",
    "Let's see how to pass multiple inputs:\n",
    "\n",
    "```\n",
    "llm_result = llm.generate([\"Tell me a joke\", \"Tell me a poem\"]*15)\n",
    "len(llm_result.generations)\n",
    "\n",
    "Output:\n",
    ">>> 30\n",
    "```\n",
    "As llm_result.generations return list, it can be accessed like standard list.\n",
    "\n",
    "If LLM provider provide usage data, LangChain will return that too but it will not work for all LLMs so consider it not standardized. As our usage is focused on OpenAI, it will work.\n",
    "\n",
    "```\n",
    "llm_result.llm_output\n",
    "```\n",
    "Results will look like:\n",
    "```\n",
    "{'token_usage': {'completion_tokens': 3903,\n",
    "  'total_tokens': 4023,\n",
    "  'prompt_tokens': 120}}\n",
    "```\n",
    "\n",
    "Further, you can also count number of tokens in query:\n",
    "\n",
    "```\n",
    "llm.get_num_tokens(\"what a joke\")\n",
    ">>> 3\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How (and why) to use the fake LLM\n",
    "We expose a fake LLM class that can be used for testing. This allows you to mock out calls to the LLM and simulate what would happen if the LLM responded in a certain way.\n",
    "\n",
    "Here is code example:\n",
    "\n",
    "```\n",
    "from langchain.llms.fake import FakeListLLM\n",
    "\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "tools = load_tools([\"python_repl\"])\n",
    "responses=[\n",
    "    \"Action: Python REPL\\nAction Input: print(2 + 2)\",\n",
    "    \"Final Answer: 4\"\n",
    "]\n",
    "llm = FakeListLLM(responses=responses)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "agent.run(\"whats 2 + 2\")\n",
    "\n",
    "Output:\n",
    "> Entering new AgentExecutor chain...\n",
    "Action: Python REPL\n",
    "Action Input: print(2 + 2)\n",
    "Observation: 4\n",
    "\n",
    "Thought:Final Answer: 4\n",
    "\n",
    "> Finished chain.\n",
    "\n",
    "'4'\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not writing about `LLM Wrappers`, `Cache LLM calls`, `Stream LLM and chat response` as my team does not require that ATM, will update if required in future. <br>\n",
    "Let's talk about token usage.\n",
    "\n",
    "```\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-002\", n=2, best_of=2)\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm(\"Tell me a joke\")\n",
    "    print(cb)\n",
    "```\n",
    "Output will be:\n",
    "\n",
    "```\n",
    ">>> Tokens Used: 42\n",
    "...  \tPrompt Tokens: 4\n",
    "... \tCompletion Tokens: 38\n",
    "... Successful Requests: 1\n",
    "... Total Cost (USD): $0.00084\n",
    "```\n",
    "\n",
    "Anything inside the context manager will get tracked. Hereâ€™s an example of using it to track multiple calls in sequence.\n",
    "\n",
    "```\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm(\"Tell me a joke\")\n",
    "    result2 = llm(\"Tell me a joke\")\n",
    "    print(cb.total_tokens)\n",
    "```\n",
    "Results:\n",
    "\n",
    "```\n",
    ">>> 91\n",
    "```\n",
    "\n",
    "Sometimes, our code will be using multiple steps, tracking is simple for that too.\n",
    "Here is code:\n",
    "\n",
    "```\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    response = agent.run(\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\")\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost (USD): ${cb.total_cost}\")\n",
    "```\n",
    "\n",
    "Results will go something like below:\n",
    "\n",
    "```\n",
    "> Entering new AgentExecutor chain...\n",
    " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\n",
    "Action: Search\n",
    "Action Input: \"Olivia Wilde boyfriend\"\n",
    "Observation: Sudeikis and Wilde's relationship ended in November 2020. Wilde was publicly served with court documents regarding child custody while she was presenting Don't Worry Darling at CinemaCon 2022. In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling.\n",
    "Thought: I need to find out Harry Styles' age.\n",
    "Action: Search\n",
    "Action Input: \"Harry Styles age\"\n",
    "Observation: 29 years\n",
    "Thought: I need to calculate 29 raised to the 0.23 power.\n",
    "Action: Calculator\n",
    "Action Input: 29^0.23\n",
    "Observation: Answer: 2.169459462491557\n",
    "\n",
    "Thought: I now know the final answer.\n",
    "Final Answer: Harry Styles, Olivia Wilde's boyfriend, is 29 years old and his age raised to the 0.23 power is 2.169459462491557.\n",
    "\n",
    "> Finished chain.\n",
    "Total Tokens: 1506\n",
    "Prompt Tokens: 1350\n",
    "Completion Tokens: 156\n",
    "Total Cost (USD): $0.03012\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration guide\n",
    "Although entire guideline is for number of models, I am writing for the models my team will use, ie. OpenAI, Cohere, Hugging face Hub and Hugging face local pipelines\n",
    "\n",
    "### Cohere\n",
    "\n",
    "```\n",
    ">>> pip install cohere\n",
    "```\n",
    "After installation, it is standard procedure.\n",
    "```\n",
    "# get a new token: https://dashboard.cohere.ai/\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "COHERE_API_KEY = getpass()\n",
    "\n",
    "from langchain.llms import Cohere\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm = Cohere(cohere_api_key=COHERE_API_KEY)\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "\n",
    "llm_chain.run(question)\n",
    "```\n",
    "\n",
    "Results will go like:\n",
    "\n",
    "```\n",
    "\" Let's start with the year that Justin Beiber was born. You know that he was born in 1994. We have to go back one year. 1993.\\n\\n1993 was the year that the Dallas Cowboys won the Super Bowl. They won over the Buffalo Bills in Super Bowl 26.\\n\\nNow, let's do it backwards. According to our information, the Green Bay Packers last won the Super Bowl in the 2010-2011 season. Now, we can't go back in time, so let's go from 2011 when the Packers won the Super Bowl, back to 1984. That is the year that the Packers won the Super Bowl over the Raiders.\\n\\nSo, we have the year that Justin Beiber was born, 1994, and the year that the Packers last won the Super Bowl, 2011, and now we have to go in the middle, 1986. That is the year that the New York Giants won the Super Bowl over the Denver Broncos. The Giants won Super Bowl 21.\\n\\nThe New York Giants won the Super Bowl in 1986. This means that the Green Bay Packers won the Super Bowl in 2011.\\n\\nDid you get it right? If you are still a bit confused, just try to go back to the question again and review the answer\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
